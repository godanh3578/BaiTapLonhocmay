# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from scipy import stats
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ƒê·ªçc d·ªØ li·ªáu train v√† test
try:
    # Th·ª≠ ƒë·ªçc t·ª´ c√°c ƒë∆∞·ªùng d·∫´n ph·ªï bi·∫øn tr√™n Kaggle
    possible_paths = [
        '/kaggle/input/mushroom-classification/mushrooms.csv',
        '/kaggle/input/mushroom-classification/train.csv',
        '/kaggle/input/uci-mushroom-dataset/mushrooms.csv',
        '/kaggle/input/test-csv/test.csv',
        '/kaggle/input/train-csv/train.csv'
    ]
    
    train_df = None
    test_df = None
    
    for path in possible_paths:
        try:
            if 'train' in path or 'mushroom' in path:
                train_df = pd.read_csv(path)
                print(f"ƒê·ªçc train data th√†nh c√¥ng t·ª´: {path}")
            elif 'test' in path:
                test_df = pd.read_csv(path)
                print(f"ƒê·ªçc test data th√†nh c√¥ng t·ª´: {path}")
        except:
            continue
    
    # N·∫øu kh√¥ng t√¨m th·∫•y file test, t·∫°o t·ª´ train data
    if train_df is not None and test_df is None:
        test_df = train_df.sample(100, random_state=42).copy()
        if 'class' in test_df.columns:
            test_df.drop('class', axis=1, inplace=True)
        print("T·∫°o test data t·ª´ train data")
    
except Exception as e:
    print(f"L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {e}")
    # Fallback: t·∫°o d·ªØ li·ªáu m·∫´u
    print("T·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo...")
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
    feature_names = [f'feature_{i}' for i in range(20)]
    train_df = pd.DataFrame(X, columns=feature_names)
    train_df['class'] = ['e' if label == 0 else 'p' for label in y]
    test_df = pd.DataFrame(np.random.randn(100, 20), columns=feature_names)

print(f"K√≠ch th∆∞·ªõc t·∫≠p train: {train_df.shape}")
print(f"K√≠ch th∆∞·ªõc t·∫≠p test: {test_df.shape}")

# Ki·ªÉm tra xem c√≥ c·ªôt 'class' kh√¥ng
if 'class' not in train_df.columns:
    # N·∫øu kh√¥ng c√≥ c·ªôt class, t·∫°o c·ªôt m·ª•c ti√™u gi·∫£
    print("Kh√¥ng t√¨m th·∫•y c·ªôt 'class', t·∫°o c·ªôt m·ª•c ti√™u gi·∫£...")
    train_df['class'] = np.random.choice(['e', 'p'], size=len(train_df))

# =============================================================================
# 1. KH√ÅM PH√Å V√Ä PH√ÇN T√çCH D·ªÆ LI·ªÜU (EDA)
# =============================================================================

print("="*60)
print("1. KH√ÅM PH√Å V√Ä PH√ÇN T√çCH D·ªÆ LI·ªÜU (EDA)")
print("="*60)

# Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu
print("1.1. Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu:")
print(f"S·ªë l∆∞·ª£ng m·∫´u: {train_df.shape[0]}")
print(f"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng: {train_df.shape[1]}")
print("\nTh√¥ng tin v·ªÅ c√°c c·ªôt:")
print(train_df.info())
print("\n5 d√≤ng ƒë·∫ßu c·ªßa dataset:")
print(train_df.head())

# Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng m·ª•c ti√™u
print("\n1.2. Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng m·ª•c ti√™u (class):")
print("Ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u:")
print(train_df['class'].value_counts())

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 6))
sns.countplot(data=train_df, x='class')
plt.title('Ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u (class)')
plt.xlabel('Lo·∫°i n·∫•m')
plt.ylabel('S·ªë l∆∞·ª£ng')
plt.show()

# Ph√¢n t√≠ch c√°c ƒë·∫∑c tr∆∞ng ri√™ng l·∫ª
print("\n1.3. Ph√¢n t√≠ch c√°c ƒë·∫∑c tr∆∞ng ri√™ng l·∫ª:")

# Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng s·ªë (n·∫øu c√≥)
numeric_columns = train_df.select_dtypes(include=[np.number]).columns
if len(numeric_columns) > 0:
    print("Th·ªëng k√™ m√¥ t·∫£ cho c√°c ƒë·∫∑c tr∆∞ng s·ªë:")
    print(train_df[numeric_columns].describe())
else:
    print("Kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng s·ªë trong dataset.")

# Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i
categorical_columns = train_df.select_dtypes(include=['object']).columns
categorical_columns = [col for col in categorical_columns if col != 'class']
print(f"\nC√≥ {len(categorical_columns)} ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i:")

# V·∫Ω bi·ªÉu ƒë·ªì cho m·ªôt s·ªë ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i quan tr·ªçng
important_features = ['cap-shape', 'cap-color', 'odor', 'gill-color', 'population', 'habitat']
available_features = [f for f in important_features if f in train_df.columns]

if available_features:
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    for i, feature in enumerate(available_features):
        if i < len(axes):
            value_counts = train_df[feature].value_counts()
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng categories ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp
            if len(value_counts) > 10:
                top_10 = value_counts.head(10)
                axes[i].bar(range(len(top_10)), top_10.values)
                axes[i].set_xticks(range(len(top_10)))
                axes[i].set_xticklabels(top_10.index, rotation=45)
            else:
                axes[i].bar(range(len(value_counts)), value_counts.values)
                axes[i].set_xticks(range(len(value_counts)))
                axes[i].set_xticklabels(value_counts.index, rotation=45)
            axes[i].set_title(f'Ph√¢n ph·ªëi c·ªßa {feature}')
    
    # ·∫®n c√°c subplot kh√¥ng s·ª≠ d·ª•ng
    for i in range(len(available_features), len(axes)):
        axes[i].set_visible(False)
        
    plt.tight_layout()
    plt.show()
else:
    # V·∫Ω bi·ªÉu ƒë·ªì cho 6 ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i ƒë·∫ßu ti√™n
    if len(categorical_columns) > 0:
        features_to_plot = categorical_columns[:6]
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, feature in enumerate(features_to_plot):
            value_counts = train_df[feature].value_counts()
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng categories
            if len(value_counts) > 10:
                top_10 = value_counts.head(10)
                axes[i].bar(range(len(top_10)), top_10.values)
                axes[i].set_xticks(range(len(top_10)))
                axes[i].set_xticklabels(top_10.index, rotation=45)
            else:
                axes[i].bar(range(len(value_counts)), value_counts.values)
                axes[i].set_xticks(range(len(value_counts)))
                axes[i].set_xticklabels(value_counts.index, rotation=45)
            axes[i].set_title(f'Ph√¢n ph·ªëi c·ªßa {feature}')
        
        # ·∫®n c√°c subplot kh√¥ng s·ª≠ d·ª•ng
        for i in range(len(features_to_plot), len(axes)):
            axes[i].set_visible(False)
            
        plt.tight_layout()
        plt.show()

# Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
print("\n1.4. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu:")
missing_data = train_df.isnull().sum()
missing_columns = missing_data[missing_data > 0]
if len(missing_columns) > 0:
    print("D·ªØ li·ªáu thi·∫øu trong t·ª´ng c·ªôt:")
    print(missing_columns)
else:
    print("Kh√¥ng c√≥ d·ªØ li·ªáu thi·∫øu trong dataset.")

# =============================================================================
# 2. TI·ªÄN X·ª¨ L√ù V√Ä K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG
# =============================================================================

print("\n" + "="*50)
print("2. TI·ªÄN X·ª¨ L√ù V√Ä K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG")
print("="*50)

# T√°ch ƒë·∫∑c tr∆∞ng v√† nh√£n
X = train_df.drop('class', axis=1)
y = train_df['class']

# M√£ h√≥a nh√£n (e: edible - ƒÉn ƒë∆∞·ª£c, p: poisonous - ƒë·ªôc)
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # e -> 0, p -> 1

# Chia t·∫≠p train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: {X_train.shape}")
print(f"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra: {X_test.shape}")

# =============================================================================
# LU·ªíNG A: K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG TRUY·ªÄN TH·ªêNG
# =============================================================================

print("\n" + "-"*30)
print("LU·ªíNG A: K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG TRUY·ªÄN TH·ªêNG")
print("-"*30)

# X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu (n·∫øu c√≥)
print("2.A.1. X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu:")
if len(missing_columns) > 0:
    # ƒêi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng mode cho categorical features
    for column in missing_columns.index:
        if column in X.columns:
            mode_value = X[column].mode()[0]
            X_train[column] = X_train[column].fillna(mode_value)
            X_test[column] = X_test[column].fillna(mode_value)
    print("ƒê√£ x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu")
else:
    print("Kh√¥ng c√≥ d·ªØ li·ªáu thi·∫øu trong dataset, n√™n b·ªè qua b∆∞·ªõc n√†y.")

# M√£ h√≥a ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i
print("\n2.A.2. M√£ h√≥a ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i:")
print("L√Ω do: S·ª≠ d·ª•ng One-Hot Encoding v√¨ t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng ƒë·ªÅu l√† ƒë·ªãnh danh (nominal)")

# One-Hot Encoding
X_train_encoded = pd.get_dummies(X_train, prefix_sep='_')
X_test_encoded = pd.get_dummies(X_test, prefix_sep='_')

# ƒê·∫£m b·∫£o t·∫≠p test c√≥ c√πng c·∫•u tr√∫c v·ªõi t·∫≠p train
missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)
for col in missing_cols:
    X_test_encoded[col] = 0

extra_cols = set(X_test_encoded.columns) - set(X_train_encoded.columns)
X_test_encoded = X_test_encoded.drop(columns=extra_cols)

# S·∫Øp x·∫øp columns theo th·ª© t·ª±
X_test_encoded = X_test_encoded[X_train_encoded.columns]

print(f"K√≠ch th∆∞·ªõc sau One-Hot Encoding - Train: {X_train_encoded.shape}")
print(f"K√≠ch th∆∞·ªõc sau One-Hot Encoding - Test: {X_test_encoded.shape}")

# Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng
print("\n2.A.3. Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng:")
print("L√Ω do: S·ª≠ d·ª•ng StandardScaler v√¨ d·ªØ li·ªáu sau One-Hot Encoding c√≥ ph√¢n ph·ªëi kh√°c nhau")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

print("Ho√†n th√†nh chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng")

# =============================================================================
# LU·ªíNG B: H·ªåC ƒê·∫∂C TR∆ØNG B·∫∞NG AUTOENCODER
# =============================================================================

print("\n" + "-"*30)
print("LU·ªíNG B: H·ªåC ƒê·∫∂C TR∆ØNG B·∫∞NG AUTOENCODER")
print("-"*30)

# Chu·∫©n b·ªã d·ªØ li·ªáu cho Autoencoder
# S·ª≠ d·ª•ng Label Encoding cho c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i
label_encoders = {}
X_train_encoded_ae = X_train.copy()
X_test_encoded_ae = X_test.copy()

for column in X_train.columns:
    le_feature = LabelEncoder()
    # X·ª≠ l√Ω c√°c gi√° tr·ªã m·ªõi trong test set
    unique_train = set(X_train[column].unique())
    unique_test = set(X_test[column].unique())
    
    # K·∫øt h·ª£p t·∫•t c·∫£ gi√° tr·ªã
    all_values = list(unique_train.union(unique_test))
    le_feature.fit(all_values)
    
    X_train_encoded_ae[column] = le_feature.transform(X_train[column])
    X_test_encoded_ae[column] = le_feature.transform(X_test[column])
    label_encoders[column] = le_feature

# Chu·∫©n h√≥a d·ªØ li·ªáu
scaler_ae = StandardScaler()
X_train_scaled_ae = scaler_ae.fit_transform(X_train_encoded_ae)
X_test_scaled_ae = scaler_ae.transform(X_test_encoded_ae)

print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu cho Autoencoder: {X_train_scaled_ae.shape}")

# X√¢y d·ª±ng Autoencoder
print("\n2.B.1. Ki·∫øn tr√∫c Autoencoder:")
input_dim = X_train_scaled_ae.shape[1]
encoding_dim = min(20, input_dim // 2)  # ƒê·∫£m b·∫£o encoding_dim kh√¥ng qu√° l·ªõn

print("L√Ω do ch·ªçn k√≠ch th∆∞·ªõc kh√¥ng gian ·∫©n:")
print(f"- D·ªØ li·ªáu g·ªëc c√≥ {input_dim} ƒë·∫∑c tr∆∞ng")
print(f"- Ch·ªçn encoding_dim = {encoding_dim} ƒë·ªÉ c√¢n b·∫±ng gi·ªØa kh·∫£ nƒÉng n√©n v√† b·∫£o to√†n th√¥ng tin")
print(f"- T·ª∑ l·ªá n√©n: {input_dim} -> {encoding_dim} (gi·∫£m {((input_dim-encoding_dim)/input_dim)*100:.1f}%)")

# ƒê·ªãnh nghƒ©a Autoencoder
input_layer = Input(shape=(input_dim,))
encoder = Dense(64, activation='relu')(input_layer)
encoder = Dense(32, activation='relu')(encoder)
bottleneck = Dense(encoding_dim, activation='relu')(encoder)

decoder = Dense(32, activation='relu')(bottleneck)
decoder = Dense(64, activation='relu')(decoder)
output_layer = Dense(input_dim, activation='sigmoid')(decoder)

autoencoder = Model(inputs=input_layer, outputs=output_layer)
encoder_model = Model(inputs=input_layer, outputs=bottleneck)

autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

print("\n2.B.2. Qu√° tr√¨nh hu·∫•n luy·ªán Autoencoder:")
print("H√†m m·∫•t m√°t: Mean Squared Error (MSE)")
print("Thu·∫≠t to√°n t·ªëi ∆∞u: Adam v·ªõi learning_rate=0.001")
print("S·ªë epochs: 30")  # Gi·∫£m ƒë·ªÉ ch·∫°y nhanh h∆°n
print("K√≠ch th∆∞·ªõc batch: 32")

# Hu·∫•n luy·ªán Autoencoder
history = autoencoder.fit(
    X_train_scaled_ae, X_train_scaled_ae,
    epochs=30,
    batch_size=32,
    shuffle=True,
    validation_data=(X_test_scaled_ae, X_test_scaled_ae),
    verbose=1  # Hi·ªÉn th·ªã ti·∫øn tr√¨nh
)

# V·∫Ω ƒë·ªì th·ªã qu√° tr√¨nh hu·∫•n luy·ªán
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Autoencoder Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

print("\n2.B.3. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng:")
print("S·ª≠ d·ª•ng ph·∫ßn Encoder ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu g·ªëc")

# Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ Autoencoder
X_train_ae_features = encoder_model.predict(X_train_scaled_ae, verbose=0)
X_test_ae_features = encoder_model.predict(X_test_scaled_ae, verbose=0)

print(f"K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng sau Autoencoder - Train: {X_train_ae_features.shape}")
print(f"K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng sau Autoencoder - Test: {X_test_ae_features.shape}")

# =============================================================================
# 3. HU·∫§N LUY·ªÜN M√î H√åNH
# =============================================================================

print("\n" + "="*50)
print("3. HU·∫§N LUY·ªÜN M√î H√åNH")
print("="*50)

# S·ª≠ d·ª•ng RandomForestClassifier ƒë·ªÉ so s√°nh
print("S·ª≠ d·ª•ng RandomForestClassifier ƒë·ªÉ so s√°nh hi·ªáu qu·∫£ c·ªßa hai ph∆∞∆°ng ph√°p")

# Hu·∫•n luy·ªán tr√™n ƒë·∫∑c tr∆∞ng truy·ªÅn th·ªëng (Lu·ªìng A)
print("\n3.1. Hu·∫•n luy·ªán tr√™n ƒë·∫∑c tr∆∞ng truy·ªÅn th·ªëng (Lu·ªìng A):")
rf_traditional = RandomForestClassifier(n_estimators=100, max_depth=10, 
                                       min_samples_split=5, min_samples_leaf=2,
                                       random_state=42, n_jobs=-1)
rf_traditional.fit(X_train_scaled, y_train)
y_pred_traditional = rf_traditional.predict(X_test_scaled)

# Hu·∫•n luy·ªán tr√™n ƒë·∫∑c tr∆∞ng t·ª´ Autoencoder (Lu·ªìng B)
print("3.2. Hu·∫•n luy·ªán tr√™n ƒë·∫∑c tr∆∞ng t·ª´ Autoencoder (Lu·ªìng B):")
rf_autoencoder = RandomForestClassifier(n_estimators=100, max_depth=10,
                                       min_samples_split=5, min_samples_leaf=2,
                                       random_state=42, n_jobs=-1)
rf_autoencoder.fit(X_train_ae_features, y_train)
y_pred_autoencoder = rf_autoencoder.predict(X_test_ae_features)

# =============================================================================
# 4. ƒê√ÅNH GI√Å V√Ä PH√ÇN T√çCH K·∫æT QU·∫¢
# =============================================================================

print("\n" + "="*50)
print("4. ƒê√ÅNH GI√Å V√Ä PH√ÇN T√çCH K·∫æT QU·∫¢")
print("="*50)

# ƒê√°nh gi√° k·∫øt qu·∫£
accuracy_traditional = accuracy_score(y_test, y_pred_traditional)
accuracy_autoencoder = accuracy_score(y_test, y_pred_autoencoder)

print("4.1. ƒê·ªô ch√≠nh x√°c (Accuracy):")
print(f"Lu·ªìng A (ƒê·∫∑c tr∆∞ng truy·ªÅn th·ªëng): {accuracy_traditional:.4f}")
print(f"Lu·ªìng B (ƒê·∫∑c tr∆∞ng t·ª´ Autoencoder): {accuracy_autoencoder:.4f}")

# B√°o c√°o chi ti·∫øt
print("\n4.2. B√°o c√°o chi ti·∫øt - Lu·ªìng A (ƒê·∫∑c tr∆∞ng truy·ªÅn th·ªëng):")
print(classification_report(y_test, y_pred_traditional, target_names=le.classes_))

print("\n4.3. B√°o c√°o chi ti·∫øt - Lu·ªìng B (ƒê·∫∑c tr∆∞ng t·ª´ Autoencoder):")
print(classification_report(y_test, y_pred_autoencoder, target_names=le.classes_))

# Ma tr·∫≠n nh·∫ßm l·∫´n
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Lu·ªìng A
cm_traditional = confusion_matrix(y_test, y_pred_traditional)
sns.heatmap(cm_traditional, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[0])
axes[0].set_title('Ma tr·∫≠n nh·∫ßm l·∫´n - Lu·ªìng A\n(ƒê·∫∑c tr∆∞ng truy·ªÅn th·ªëng)')
axes[0].set_xlabel('D·ª± ƒëo√°n')
axes[0].set_ylabel('Th·ª±c t·∫ø')

# Lu·ªìng B
cm_autoencoder = confusion_matrix(y_test, y_pred_autoencoder)
sns.heatmap(cm_autoencoder, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[1])
axes[1].set_title('Ma tr·∫≠n nh·∫ßm l·∫´n - Lu·ªìng B\n(ƒê·∫∑c tr∆∞ng t·ª´ Autoencoder)')
axes[1].set_xlabel('D·ª± ƒëo√°n')
axes[1].set_ylabel('Th·ª±c t·∫ø')

plt.tight_layout()
plt.show()

# =============================================================================
# 5. ƒê√ÅNH GI√Å ƒê·ªò ·ªîN ƒê·ªäNH QUA NHI·ªÄU L·∫¶N HU·∫§N LUY·ªÜN
# =============================================================================

print("\n" + "="*60)
print("5. ƒê√ÅNH GI√Å ƒê·ªò ·ªîN ƒê·ªäNH QUA NHI·ªÄU L·∫¶N HU·∫§N LUY·ªÜN")
print("="*60)

# Chu·∫©n b·ªã d·ªØ li·ªáu t·ªïng th·ªÉ
X_encoded = pd.get_dummies(X, prefix_sep='_')
scaler_global = StandardScaler()
X_scaled_global = scaler_global.fit_transform(X_encoded)

# Chu·∫©n b·ªã d·ªØ li·ªáu cho Autoencoder
X_encoded_ae = X.copy()
for column in X.columns:
    le_col = LabelEncoder()
    X_encoded_ae[column] = le_col.fit_transform(X[column])
X_scaled_ae_global = StandardScaler().fit_transform(X_encoded_ae)

# Hu·∫•n luy·ªán Autoencoder tr√™n to√†n b·ªô d·ªØ li·ªáu
input_dim = X_scaled_ae_global.shape[1]
encoding_dim = min(20, input_dim // 2)

input_layer = Input(shape=(input_dim,))
encoder = Dense(64, activation='relu')(input_layer)
encoder = Dense(32, activation='relu')(encoder)
bottleneck = Dense(encoding_dim, activation='relu')(encoder)
decoder = Dense(32, activation='relu')(bottleneck)
decoder = Dense(64, activation='relu')(decoder)
output_layer = Dense(input_dim, activation='sigmoid')(decoder)

autoencoder_global = Model(inputs=input_layer, outputs=output_layer)
encoder_global = Model(inputs=input_layer, outputs=bottleneck)
autoencoder_global.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
autoencoder_global.fit(X_scaled_ae_global, X_scaled_ae_global, 
                      epochs=30, batch_size=32, verbose=0)

X_ae_features_global = encoder_global.predict(X_scaled_ae_global, verbose=0)

# Th·ª≠ nghi·ªám v·ªõi nhi·ªÅu random seed
n_iterations = 5  # Gi·∫£m s·ªë l·∫ßn l·∫∑p ƒë·ªÉ ch·∫°y nhanh h∆°n
traditional_accuracies = []
autoencoder_accuracies = []
traditional_f1_scores = []
autoencoder_f1_scores = []

print(f"\nTh·ª±c hi·ªán {n_iterations} l·∫ßn hu·∫•n luy·ªán v·ªõi c√°c random seed kh√°c nhau...")

for i in tqdm(range(n_iterations)):
    # Chia d·ªØ li·ªáu v·ªõi random seed kh√°c nhau
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled_global, y_encoded, test_size=0.2, 
        random_state=i, stratify=y_encoded
    )
    
    X_train_ae, X_test_ae, y_train_ae, y_test_ae = train_test_split(
        X_ae_features_global, y_encoded, test_size=0.2, 
        random_state=i, stratify=y_encoded
    )
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh truy·ªÅn th·ªëng
    rf_traditional = RandomForestClassifier(n_estimators=50, random_state=42)  # Gi·∫£m ƒë·ªÉ ch·∫°y nhanh
    rf_traditional.fit(X_train, y_train)
    y_pred_traditional = rf_traditional.predict(X_test)
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh Autoencoder
    rf_autoencoder = RandomForestClassifier(n_estimators=50, random_state=42)  # Gi·∫£m ƒë·ªÉ ch·∫°y nhanh
    rf_autoencoder.fit(X_train_ae, y_train_ae)
    y_pred_autoencoder = rf_autoencoder.predict(X_test_ae)
    
    # T√≠nh to√°n metrics
    acc_trad = accuracy_score(y_test, y_pred_traditional)
    acc_ae = accuracy_score(y_test_ae, y_pred_autoencoder)
    
    f1_trad = f1_score(y_test, y_pred_traditional, average='weighted')
    f1_ae = f1_score(y_test_ae, y_pred_autoencoder, average='weighted')
    
    traditional_accuracies.append(acc_trad)
    autoencoder_accuracies.append(acc_ae)
    traditional_f1_scores.append(f1_trad)
    autoencoder_f1_scores.append(f1_ae)

# Ph√¢n t√≠ch k·∫øt qu·∫£
print("\n" + "="*50)
print("PH√ÇN T√çCH ƒê·ªò ·ªîN ƒê·ªäNH")
print("="*50)

# Th·ªëng k√™ m√¥ t·∫£
traditional_acc_mean = np.mean(traditional_accuracies)
traditional_acc_std = np.std(traditional_accuracies)
autoencoder_acc_mean = np.mean(autoencoder_accuracies)
autoencoder_acc_std = np.std(autoencoder_accuracies)

traditional_f1_mean = np.mean(traditional_f1_scores)
traditional_f1_std = np.std(traditional_f1_scores)
autoencoder_f1_mean = np.mean(autoencoder_f1_scores)
autoencoder_f1_std = np.std(autoencoder_f1_scores)

print("\nTH·ªêNG K√ä ƒê·ªò CH√çNH X√ÅC (Accuracy):")
print(f"Lu·ªìng A - Truy·ªÅn th·ªëng: {traditional_acc_mean:.4f} ¬± {traditional_acc_std:.4f}")
print(f"Lu·ªìng B - Autoencoder:  {autoencoder_acc_mean:.4f} ¬± {autoencoder_acc_std:.4f}")

print("\nTH·ªêNG K√ä F1-SCORE:")
print(f"Lu·ªìng A - Truy·ªÅn th·ªëng: {traditional_f1_mean:.4f} ¬± {traditional_f1_std:.4f}")
print(f"Lu·ªìng B - Autoencoder:  {autoencoder_f1_mean:.4f} ¬± {autoencoder_f1_std:.4f}")

# Ki·ªÉm ƒë·ªãnh th·ªëng k√™
try:
    t_stat_acc, p_value_acc = stats.ttest_rel(traditional_accuracies, autoencoder_accuracies)
    t_stat_f1, p_value_f1 = stats.ttest_rel(traditional_f1_scores, autoencoder_f1_scores)

    print(f"\nKI·ªÇM ƒê·ªäNH T-TEST (paired):")
    print(f"Accuracy - t-statistic: {t_stat_acc:.4f}, p-value: {p_value_acc:.4f}")
    print(f"F1-score - t-statistic: {t_stat_f1:.4f}, p-value: {p_value_f1:.4f}")

    if p_value_acc < 0.05:
        if traditional_acc_mean > autoencoder_acc_mean:
            print("‚Üí S·ª± kh√°c bi·ªát v·ªÅ Accuracy c√≥ √Ω nghƒ©a th·ªëng k√™ (Lu·ªìng A t·ªët h∆°n)")
        else:
            print("‚Üí S·ª± kh√°c bi·ªát v·ªÅ Accuracy c√≥ √Ω nghƒ©a th·ªëng k√™ (Lu·ªìng B t·ªët h∆°n)")
    else:
        print("‚Üí Kh√¥ng c√≥ s·ª± kh√°c bi·ªát c√≥ √Ω nghƒ©a th·ªëng k√™ v·ªÅ Accuracy")
except:
    print("Kh√¥ng th·ªÉ th·ª±c hi·ªán ki·ªÉm ƒë·ªãnh th·ªëng k√™ do s·ªë l∆∞·ª£ng m·∫´u √≠t")

# =============================================================================
# 6. SO S√ÅNH V√Ä K·∫æT LU·∫¨N
# =============================================================================

print("\n" + "="*50)
print("6. SO S√ÅNH V√Ä K·∫æT LU·∫¨N")
print("="*50)

# So s√°nh k·∫øt qu·∫£
print("6.1. So s√°nh k·∫øt qu·∫£:")
print(f"ƒê·ªô ch√≠nh x√°c Lu·ªìng A: {accuracy_traditional:.4f}")
print(f"ƒê·ªô ch√≠nh x√°c Lu·ªìng B: {accuracy_autoencoder:.4f}")

if accuracy_traditional > accuracy_autoencoder:
    difference = accuracy_traditional - accuracy_autoencoder
    print(f"Lu·ªìng A t·ªët h∆°n Lu·ªìng B: {difference:.4f}")
    best_method = 'traditional'
elif accuracy_autoencoder > accuracy_traditional:
    difference = accuracy_autoencoder - accuracy_traditional
    print(f"Lu·ªìng B t·ªët h∆°n Lu·ªìng A: {difference:.4f}")
    best_method = 'autoencoder'
else:
    print("Hai ph∆∞∆°ng ph√°p cho k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng")
    best_method = 'traditional'  # M·∫∑c ƒë·ªãnh

# =============================================================================
# 7. XU·∫§T FILE SUBMISSION CHO KAGGLE
# =============================================================================

print("\n" + "="*50)
print("7. XU·∫§T FILE SUBMISSION CHO KAGGLE")
print("="*50)

# Chu·∫©n b·ªã d·ªØ li·ªáu test th·ª±c t·∫ø
if 'id' in test_df.columns:
    test_ids = test_df['id']
    X_test_final = test_df.drop('id', axis=1)
else:
    test_ids = range(len(test_df))
    X_test_final = test_df.copy()

print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu test: {X_test_final.shape}")

# Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t
if best_method == 'traditional':
    print("üìä S·ª≠ d·ª•ng m√¥ h√¨nh TRUY·ªÄN TH·ªêNG ƒë·ªÉ t·∫°o submission...")

    # Ti·ªÅn x·ª≠ l√Ω One-Hot Encoding
    X_full = pd.get_dummies(X, prefix_sep='_')
    X_test_final_enc = pd.get_dummies(X_test_final, prefix_sep='_')
    
    # ƒê·∫£m b·∫£o test data c√≥ c√πng features v·ªõi train data
    missing_cols = set(X_full.columns) - set(X_test_final_enc.columns)
    for col in missing_cols:
        X_test_final_enc[col] = 0
    
    extra_cols = set(X_test_final_enc.columns) - set(X_full.columns)
    X_test_final_enc = X_test_final_enc.drop(columns=extra_cols)
    
    X_test_final_enc = X_test_final_enc[X_full.columns]

    scaler_final = StandardScaler()
    X_full_scaled = scaler_final.fit_transform(X_full)
    X_test_final_scaled = scaler_final.transform(X_test_final_enc)

    final_model = RandomForestClassifier(n_estimators=100, max_depth=10,
                                        min_samples_split=5, min_samples_leaf=2,
                                        random_state=42, n_jobs=-1)
    final_model.fit(X_full_scaled, y_encoded)

    test_predictions = final_model.predict(X_test_final_scaled)

else:
    print("ü§ñ S·ª≠ d·ª•ng m√¥ h√¨nh AUTOENCODER ƒë·ªÉ t·∫°o submission...")

    # Label Encoding cho test
    X_full_ae = X.copy()
    X_test_final_ae = X_test_final.copy()
    
    for column in X_full_ae.columns:
        le_col = LabelEncoder()
        unique_train = set(X_full_ae[column].unique())
        unique_test = set(X_test_final_ae[column].unique())
        
        # K·∫øt h·ª£p t·∫•t c·∫£ gi√° tr·ªã
        all_values = list(unique_train.union(unique_test))
        le_col.fit(all_values)
        
        X_full_ae[column] = le_col.transform(X_full_ae[column])
        X_test_final_ae[column] = le_col.transform(X_test_final_ae[column])

    scaler_final_ae = StandardScaler()
    X_full_scaled_ae = scaler_final_ae.fit_transform(X_full_ae)
    X_test_final_scaled_ae = scaler_final_ae.transform(X_test_final_ae)

    # Train autoencoder cu·ªëi c√πng
    input_layer_final = Input(shape=(X_full_scaled_ae.shape[1],))
    encoder_final = Dense(64, activation='relu')(input_layer_final)
    encoder_final = Dense(32, activation='relu')(encoder_final)
    bottleneck_final = Dense(min(20, X_full_scaled_ae.shape[1]//2), activation='relu')(encoder_final)

    decoder_final = Dense(32, activation='relu')(bottleneck_final)
    decoder_final = Dense(64, activation='relu')(decoder_final)
    output_layer_final = Dense(X_full_scaled_ae.shape[1], activation='sigmoid')(decoder_final)

    autoencoder_final = Model(inputs=input_layer_final, outputs=output_layer_final)
    encoder_model_final = Model(inputs=input_layer_final, outputs=bottleneck_final)

    autoencoder_final.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    autoencoder_final.fit(X_full_scaled_ae, X_full_scaled_ae, epochs=30, batch_size=32, verbose=0)

    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    X_full_features = encoder_model_final.predict(X_full_scaled_ae, verbose=0)
    X_test_final_features = encoder_model_final.predict(X_test_final_scaled_ae, verbose=0)

    # Train RandomForest tr√™n ƒë·∫∑c tr∆∞ng h·ªçc ƒë∆∞·ª£c
    final_model = RandomForestClassifier(n_estimators=100, max_depth=10,
                                        min_samples_split=5, min_samples_leaf=2,
                                        random_state=42, n_jobs=-1)
    final_model.fit(X_full_features, y_encoded)

    test_predictions = final_model.predict(X_test_final_features)

# Chuy·ªÉn d·ª± ƒëo√°n v·ªÅ e/p
final_predictions_labels = le.inverse_transform(test_predictions)

# T·∫°o file submission
submission = pd.DataFrame({
    "id": test_ids,
    "class": final_predictions_labels
})

# Xu·∫•t ra CSV
submission.to_csv("submission.csv", index=False)

print("‚úÖ File submission.csv ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
print(f"K√≠ch th∆∞·ªõc file submission: {submission.shape}")
print